{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a18c4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c01b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(filepath):\n",
    "    tokens, labels, examples = [], [], []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    examples.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "                    tokens, labels = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    print(f\"Skipping malformed line {line_num} in {filepath}: '{line}'\")\n",
    "                    continue\n",
    "                token, tag = parts\n",
    "                tokens.append(token)\n",
    "                labels.append(tag)\n",
    "        if tokens:\n",
    "            examples.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "    return examples\n",
    "\n",
    "def load_conll_folder(folder_path):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".conll\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            examples = read_conll(full_path)\n",
    "            all_data.extend(examples)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7d17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(sentence: str):\n",
    "    # Fix UTF-8 encoding\n",
    "    sentence = unicodedata.normalize(\"NFC\", sentence)\n",
    "\n",
    "    enc = tokenizer(sentence, return_tensors=\"pt\", truncation=True)\n",
    "    enc.pop(\"token_type_ids\", None)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**enc)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    input_ids = enc[\"input_ids\"].squeeze().tolist()\n",
    "    word_ids = enc.word_ids()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    results = []\n",
    "    prev_word_id = None\n",
    "\n",
    "    for tok, pred_id, wid in zip(tokens, preds, word_ids):\n",
    "        if wid is None:\n",
    "            continue\n",
    "        \n",
    "        label = id2label[pred_id]\n",
    "\n",
    "        # ModernBERT space indicator \"Ġ\"\n",
    "        if tok.startswith(\"Ġ\"):\n",
    "            tok = \" \" + tok[1:]\n",
    "\n",
    "        # Merge subwords into readable text\n",
    "        if wid != prev_word_id:\n",
    "            results.append([tok, label])\n",
    "        else:\n",
    "            results[-1][0] += tok\n",
    "\n",
    "        prev_word_id = wid\n",
    "\n",
    "    # Final cleanup: strip spaces and remove special tokens\n",
    "    final = [(w.strip(), l) for w, l in results if w.strip() not in tokenizer.all_special_tokens]\n",
    "    return final\n",
    "\n",
    "\n",
    "def print_entities(results):\n",
    "    print(\"\\nNamed Entity Predictions:\")\n",
    "    for word, label in results:\n",
    "        if label != \"O\":\n",
    "            print(f\"{word:25} → {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6c9f7",
   "metadata": {},
   "source": [
    "# 1. Inference from an input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c3081cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust model dir as needed:\n",
    "MODEL_DIR = \"VSSA-SDSA/LT-NER-modernBERT\"  \n",
    "TRAIN_DIR = \"data/conll_train/\"\n",
    "TEST_DIR  = \"data/conll_test/\"\n",
    "\n",
    "# Load label mappings from dataset \n",
    "train_data = load_conll_folder(TRAIN_DIR)\n",
    "test_data  = load_conll_folder(TEST_DIR)\n",
    "\n",
    "unique_labels = sorted({label for ex in train_data for label in ex[\"ner_tags\"]})\n",
    "label2id = {l: i for i, l in enumerate(unique_labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "# Load tokenizer + model \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "_ = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8dea56b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Predictions:\n",
      "Proceso              --> O\n",
      "kalba                --> O\n",
      ":                    --> O\n",
      "ispanų               --> B-NATIONALITY\n",
      ".                    --> O\n"
     ]
    }
   ],
   "source": [
    "text = \"2004 m. balandžio 21 d. Europos Parlamento reglamentas Nr. 805/2004, sukuriantis reikalavimus Europos vykdomajam raštui\"\n",
    "text = \"Ieškovas: Petras Petraitis\"\n",
    "text = \"UAB „Valstybės investicinis kapitalas“ išplatino 25 mln. eurų vertės obligacijas pagal atnaujintą 400 mln. eurų vertybinių popierių programą. Nominali obligacijos vertė – 1 000 eurų, o pajamingumas – 3,119 %. Obligacijų išpirkimo data – 2029 m. rugsėjo 24 d.\"\n",
    "text = \"UAB „Valstybės investicinis kapitalas“ valdyba taip pat pritarė UAB „EPSO-G Invest“ paprastųjų vardinių nematerialiųjų 1 791 244 vnt. akcijų, kurių bendra emisijos kaina yra lygi 17 912 440 EUR, įsigijimui.\"\n",
    "text = \"Įgalioti AB „Ignitis grupė“ vadovą (su teise perįgalioti) pasirašyti sutartis dėl AB Ignitis grupė\"\n",
    "text = \"nepriklausomam nariui 4 070 Eur (neatskaičius mokesčių)\"\n",
    "text = \"Proceso kalba: ispanų.\"\n",
    "\n",
    "\n",
    "predictions = infer(text)\n",
    "\n",
    "\n",
    "print(\"\\nNER Predictions:\")\n",
    "for word, label in predictions:\n",
    "    #if label != \"O\" and len(word.strip()) > 1:\n",
    "    decoded = tokenizer.convert_tokens_to_string([word])\n",
    "    print(f\"{decoded:20} --> {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f416b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
